Llama Stack MCP Quickstart deployment completed successfully!

Components deployed:
{{- if index .Values.components "llama3-2-3b" "enabled" }}
✓ Llama 3.2-3B Model (vLLM)
{{- end }}
{{- if index .Values.components "llama-stack" "enabled" }}
✓ Llama Stack Server
{{- end }}
{{- if index .Values.components "llama-stack-playground" "enabled" }}
✓ Llama Stack Playground
{{- end }}
{{- if index .Values.components "hr-enterprise-api" "enabled" }}
✓ HR Enterprise API
{{- end }}
{{- if index .Values.components "custom-mcp-server" "enabled" }}
✓ Custom MCP Server (HR Integration)
{{- end }}
{{- if index .Values.components "mcp-swiss-transport" "enabled" }}
✓ MCP Swiss Transportation (Custom MCP)
{{- end }}
{{- if index .Values.components "mcp-vienna-transport" "enabled" }}
✓ MCP Vienna Transportation (Custom MCP)
{{- end }}
{{- if index .Values.components "mcp-riyadh-transport" "enabled" }}
✓ MCP Riyadh Bus Transportation (Custom MCP)
{{- end }}
{{- if index .Values.components "mcp-stockholm-transport" "enabled" }}
✓ MCP Stockholm Transportation (Custom MCP)
{{- end }}





To get the playground URL:
  export PLAYGROUND_URL=$(oc get route llama-stack-playground -o jsonpath='{.spec.host}' 2>/dev/null || echo "Route not found")
  echo "Playground: https://$PLAYGROUND_URL"

To check the status of all components:
  helm status llama-stack-mcp
  oc get pods 

Note: the llama-stack pod will likely be in a CrashLoopBackOff status until the llama3-2-3b-predictor pod is running
For troubleshooting:
  oc get pods
  oc logs -l app.kubernetes.io/name=llama-stack
  oc logs -l app.kubernetes.io/name=llama3-2-3b


Enjoy Llama Stack and MCP on OpenShift AI! 
